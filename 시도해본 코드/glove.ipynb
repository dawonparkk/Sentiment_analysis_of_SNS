{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "glove.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDFil7TpBSBp",
        "outputId": "a53ded3f-2bc3-466a-e89a-ad115f049af1"
      },
      "source": [
        "pip install glove-python\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting glove-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
            "\r\u001b[K     |█▎                              | 10kB 16.2MB/s eta 0:00:01\r\u001b[K     |██▌                             | 20kB 13.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 30kB 10.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 40kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 51kB 5.5MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 61kB 5.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 71kB 6.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 81kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 92kB 6.5MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 102kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 112kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 122kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 133kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 143kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 153kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 163kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 174kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 184kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 194kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 204kB 6.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 215kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 225kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 235kB 6.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 245kB 6.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 256kB 6.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 266kB 6.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from glove-python) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from glove-python) (1.4.1)\n",
            "Building wheels for collected packages: glove-python\n",
            "  Building wheel for glove-python (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for glove-python: filename=glove_python-0.1.0-cp36-cp36m-linux_x86_64.whl size=700271 sha256=6747e47c6c79010a34ed85970f43d6e6d49cc9f4873abc0f36f82f9502c2de94\n",
            "  Stored in directory: /root/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
            "Successfully built glove-python\n",
            "Installing collected packages: glove-python\n",
            "Successfully installed glove-python-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbmJfRPPBdfT"
      },
      "source": [
        "from glove import Glove, Corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JvSjTwjCUQU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coXeOWdWB427"
      },
      "source": [
        "sen = pd.read_csv('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/분노.csv', encoding = 'utf-8')\n",
        "sen2 = pd.read_csv('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/긍정.csv', encoding = 'utf-8')\n",
        "sen3 = pd.read_csv('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/중립.csv', encoding = 'utf-8')\n",
        "\n",
        "sen.columns = ['index','a']     # 컬럼명 정리\n",
        "sen = sen.loc[sen['a'].notnull(),:]   # null 처리\n",
        "only_word = list(sen['a'].str.split())  # 리스트형태로 만들기\n",
        "rage = np.repeat('2',len(only_word))\n",
        "\n",
        "\n",
        "sen2.columns = ['index','a']    # 컬럼명 정리\n",
        "sen2 = sen2.loc[sen2['a'].notnull(),:]   # null 처리\n",
        "only_word2 = list(sen2['a'].str.split())    # 리스트형태로 만들기\n",
        "positive = np.repeat('1',len(only_word2))\n",
        "\n",
        "sen3.columns = ['index','a']    # 컬럼명 정리\n",
        "sen3 = sen3.loc[sen3['a'].notnull(),:]   # null 처리\n",
        "only_word3 = list(sen3['a'].str.split())    # 리스트형태로 만들기\n",
        "neutrality = np.repeat('0',len(only_word3))\n",
        "\n",
        "total = only_word + only_word2 + only_word3\n",
        "emotion = list(rage) + list(positive) + list(neutrality)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pSVoYY1CpyI"
      },
      "source": [
        "df1 = pd.DataFrame({'text':only_word, 'tag' : rage})\n",
        "df2 = pd.DataFrame({'text':only_word2, 'tag' : positive})\n",
        "df3 = pd.DataFrame({'text':only_word3, 'tag' : neutrality})\n",
        "\n",
        "train_x1, test_x1, train_y1, test_y1 = train_test_split(df1, df1, random_state = 0, train_size = 0.8) \n",
        "train_x2, test_x2, train_y2, test_y2 = train_test_split(df2, df2, random_state = 0, train_size = 0.8) \n",
        "train_x3, test_x3, train_y3, test_y3 = train_test_split(df3, df3, random_state = 0, train_size = 0.8)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUhspFguDwcv"
      },
      "source": [
        "#df1 = pd.concat([train_x1, train_x2, train_x3])\n",
        "df2 = pd.concat([test_x1, test_x2, test_x3])\n",
        "df1 = pd.concat([train_x1, train_x2, train_x3, train_x2, train_x2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyBzrqzrDz_L"
      },
      "source": [
        "corpus = Corpus() \n",
        "corpus.fit(df1['text'],window = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgs1lJTgEMED",
        "outputId": "b36c8f7c-a97e-4f95-b677-6179f024761b"
      },
      "source": [
        "glove = Glove(no_components=100, learning_rate=0.05)\n",
        "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
        "glove.save('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/my_glove.model')\n",
        "glove.add_dictionary(corpus.dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing 20 training epochs with 4 threads\n",
            "Epoch 0\n",
            "Epoch 1\n",
            "Epoch 2\n",
            "Epoch 3\n",
            "Epoch 4\n",
            "Epoch 5\n",
            "Epoch 6\n",
            "Epoch 7\n",
            "Epoch 8\n",
            "Epoch 9\n",
            "Epoch 10\n",
            "Epoch 11\n",
            "Epoch 12\n",
            "Epoch 13\n",
            "Epoch 14\n",
            "Epoch 15\n",
            "Epoch 16\n",
            "Epoch 17\n",
            "Epoch 18\n",
            "Epoch 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80qhg9VxLcXN",
        "outputId": "068a7124-9b5e-45a3-911f-aafa96422636"
      },
      "source": [
        "glove.most_similar(\"무능\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('중앙', 0.9374183777225747),\n",
              " ('여당', 0.9353846789631499),\n",
              " ('문재인', 0.9280535640151195),\n",
              " ('좌파', 0.9177077030481601)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lHeV2blLs_t"
      },
      "source": [
        "glove.save('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/my_glove.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcII5TKDMKtv"
      },
      "source": [
        "glove_model = glove.load('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/my_glove.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "uqvLDH9xzrC2",
        "outputId": "62e3d9ae-5c85-494e-ebe0-d4b26134c1a0"
      },
      "source": [
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_file = datapath('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/my_glove.model')\n",
        "tmp_file = get_tmpfile(\"/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/test_word2vec.txt\")\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file, binary=False, encoding = 'utf-8')\n",
        "# 인코딩 에러 발생"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-ca720561f574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglove2word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# 인코딩 에러 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1437\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1438\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36many2unicode\u001b[0;34m(text, encoding, errors)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5WYOrVNHiEW"
      },
      "source": [
        "# ========================================\n",
        "# 학습시킨 glove를 사용한 deep learning\n",
        "# ========================================"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkhjpFBjLHot"
      },
      "source": [
        "# glove의 단어와 좌표 저장\n",
        "np.savetxt('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/glove-vector.tsv', glove.word_vectors, delimiter='\\t') \n",
        "with open('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/glove-metadata.tsv', 'w', encoding='utf-8') as f: \n",
        "  for key in glove.dictionary.keys(): f.write(f\"{key}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTNgdshgIDhk"
      },
      "source": [
        "# 저장된 파일들 불러오기\n",
        "df_v = pd.read_csv('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/glove-vector.tsv', sep = '\\t',header = None)\n",
        "df_w = pd.read_csv('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/glove-metadata.tsv', sep = '\\t',header = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K1RvyMI-VP_"
      },
      "source": [
        "# 불러온 파일 통합 \n",
        "a1 = pd.concat([df_w,df_v], axis = 1).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Pi5kFoQ_KLl"
      },
      "source": [
        "# txt형태로 저장\n",
        "with open('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/glove.naver.100d.txt','w', encoding = 'utf-8') as f :\n",
        "  for i in a1 :\n",
        "    f.write(f\"{' '.join(list(i.astype('str')))}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ncs9O4_NG1if",
        "outputId": "cb2a6d91-504d-4383-9caa-49d4578a2b64"
      },
      "source": [
        "# tokenizer 모듈을 사용\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(total)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X_encoded = tokenizer.texts_to_sequences(total)\n",
        "\n",
        "max_len=max(len(l) for l in X_encoded)\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hUOCFPLFt41",
        "outputId": "3ca6618b-7c3e-436a-c05c-635fc0348f14"
      },
      "source": [
        "# 사전학습 된 glove 모델 불러오기\n",
        "import numpy as np\n",
        "embedding_dict = dict()\n",
        "f = open('/content/drive/MyDrive/2조 감성분석/머신러닝 팀/학습용데이터/glove.naver.100d.txt', encoding=\"utf8\")\n",
        "\n",
        "for line in f:\n",
        "    word_vector = line.split()\n",
        "    word = word_vector[0]\n",
        "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32') # 100개의 값을 가지는 array로 변환\n",
        "    embedding_dict[word] = word_vector_arr\n",
        "f.close()\n",
        "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23042개의 Embedding vector가 있습니다.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL1NNGvzF6DZ",
        "outputId": "fa06c9c9-959e-4d0a-9126-3331dd4e1f99"
      },
      "source": [
        "# np.zeros를 사용해 비어있는 array 생성\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "np.shape(embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25385, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUTHdX5dGZLe"
      },
      "source": [
        "# glove 사전에서 단어의 좌표값을 해당되는 행에 삽입\n",
        "for word, i in tokenizer.word_index.items(): # 훈련 데이터의 단어 집합에서 단어를 1개씩 꺼내온다.\n",
        "    temp = embedding_dict.get(word) # 단어(key) 해당되는 임베딩 벡터의 100개의 값(value)를 임시 변수에 저장\n",
        "    if temp is not None:\n",
        "        embedding_matrix[i] = temp # 임수 변수의 값을 단어와 맵핑되는 인덱스의 행에 삽입"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg_NUP-MJcs-",
        "outputId": "8826a8ed-6538-4f49-b089-481672c79e5e"
      },
      "source": [
        "embedding_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
              "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
              "       [-4.35315549e-01, -5.00811934e-01, -1.71221316e-01, ...,\n",
              "        -9.11829472e-01, -3.90236735e-01, -3.50935459e-01],\n",
              "       [ 5.57623468e-02, -2.75628157e-02,  7.40528330e-02, ...,\n",
              "        -2.27616116e-01, -7.55651912e-04,  4.54227537e-01],\n",
              "       ...,\n",
              "       [-6.60108356e-03,  6.91823987e-03,  1.21658379e-02, ...,\n",
              "         3.61640309e-03,  1.48773543e-03,  7.55030429e-03],\n",
              "       [-1.43256353e-03,  1.35796587e-03, -2.88509205e-03, ...,\n",
              "         1.00190809e-03, -4.29516146e-03,  2.51416676e-03],\n",
              "       [ 3.40033090e-03, -1.05350709e-03,  5.53350183e-05, ...,\n",
              "        -2.36320146e-03,  1.11179659e-03, -4.75045340e-03]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvUvD4xdh655"
      },
      "source": [
        "# x data 문장별 정수인덱싱 후 패딩\n",
        "text_sequence = tokenizer.texts_to_sequences(total)\n",
        "\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "pad_text = pad_sequences(text_sequence, \n",
        "                         maxlen=500, \n",
        "                         truncating='post', \n",
        "                         padding='post')\n",
        "train_x, test_x, train_y, test_y = train_test_split(pad_text, \n",
        "                                                    emotion, \n",
        "                                                    test_size=0.2, \n",
        "                                                    stratify=emotion, \n",
        "                                                    random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm9-TLIsiVdS"
      },
      "source": [
        "# 원 핫 인코딩\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "labels_train = np.array(train_y).reshape(-1,1)\n",
        "labels_test = np.array(test_y).reshape(-1,1)\n",
        "onehotencoder = OneHotEncoder()\n",
        "onehotencoder.fit(labels_train)\n",
        "train_y = onehotencoder.transform(labels_train).toarray()\n",
        "test_y = onehotencoder.transform(labels_test).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUzXb5DjGh3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f90f4346-a761-4660-9f8c-2060b4344398"
      },
      "source": [
        "# 사전학습 된 glove를 keras에 적용하기\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
        "\n",
        "model = Sequential()\n",
        "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=500, trainable=False)\n",
        "model.add(e)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, input_dim = 100 ,activation = 'relu'))\n",
        "model.add(Dense(3, activation = 'softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "model.fit(train_x, train_y, epochs=10, batch_size=512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "54/54 [==============================] - 9s 169ms/step - loss: 0.8374 - acc: 0.6164\n",
            "Epoch 2/10\n",
            "54/54 [==============================] - 9s 169ms/step - loss: 0.7427 - acc: 0.6555\n",
            "Epoch 3/10\n",
            "54/54 [==============================] - 9s 170ms/step - loss: 0.7033 - acc: 0.6704\n",
            "Epoch 4/10\n",
            "54/54 [==============================] - 9s 169ms/step - loss: 0.6813 - acc: 0.6752\n",
            "Epoch 5/10\n",
            "54/54 [==============================] - 9s 170ms/step - loss: 0.6640 - acc: 0.6868\n",
            "Epoch 6/10\n",
            "54/54 [==============================] - 9s 171ms/step - loss: 0.6507 - acc: 0.6928\n",
            "Epoch 7/10\n",
            "54/54 [==============================] - 9s 171ms/step - loss: 0.6354 - acc: 0.7050\n",
            "Epoch 8/10\n",
            "54/54 [==============================] - 9s 170ms/step - loss: 0.6213 - acc: 0.7135\n",
            "Epoch 9/10\n",
            "54/54 [==============================] - 9s 171ms/step - loss: 0.6101 - acc: 0.7219\n",
            "Epoch 10/10\n",
            "54/54 [==============================] - 9s 169ms/step - loss: 0.5974 - acc: 0.7299\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f61fb26d630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlp7tW7JikLB",
        "outputId": "c0b4382b-4c11-4aab-e86a-c940ed4096c0"
      },
      "source": [
        "print(model.evaluate(test_x, test_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "215/215 [==============================] - 2s 7ms/step - loss: 0.7368 - acc: 0.6524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7368136048316956, 0.6523511409759521]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "l6mAYF-lPRzs",
        "outputId": "ac7dab3a-a109-465c-9e80-f7a5f42ea9ff"
      },
      "source": [
        "# LSTM층 추가"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "75\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-f1e67cde9982>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mINPUT_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'INPUT_LENGTH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LP3XbmkDPGyQ"
      },
      "source": [
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dropout, Dense\n",
        "\n",
        "model = Sequential()\n",
        "# 바로 이곳에 들어가는 Embedding Layer가 단어에 대하여 설정한 차원으로 변환\n",
        "model.add(Embedding(vocab_size, 100, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(Bidirectional(LSTM(128, recurrent_dropout=0.1)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(64))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_x, train_y, epochs = 10, batch_size = 512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEGEZtjyCjWJ"
      },
      "source": [
        ""
      ]
    }
  ]
}