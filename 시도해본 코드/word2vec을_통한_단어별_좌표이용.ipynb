{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSMASL2ldcsG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXsrW0M4dcsK",
        "outputId": "dac39c1a-f002-481c-d1a9-c49afe6f3d2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34343\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# 데이터 통합\n",
        "# =============================================================================\n",
        "# 분노 불러오기(우울)\n",
        "\n",
        "sen = pd.read_csv('분노.csv') \n",
        "sen.columns = ['index','a']     # 컬럼명 정리\n",
        "sen = sen.loc[sen['a'].notnull(),:]   # null 처리\n",
        "only_word = list(sen['a'].str.split())  # 리스트형태로 만들기\n",
        "rage = np.repeat('2',len(only_word))   # 문장 수만큼 감정 label 생성\n",
        "\n",
        "\n",
        "# 긍정 불러오기\n",
        "sen2 = pd.read_csv('긍정.csv')\n",
        "sen2.columns = ['index','a']    # 컬럼명 정리\n",
        "sen2 = sen2.loc[sen2['a'].notnull(),:]   # null 처리\n",
        "only_word2 = list(sen2['a'].str.split())    # 리스트형태로 만들기\n",
        "positive = np.repeat('1',len(only_word2))   # 문장 수만큼 감정 label 생성\n",
        "\n",
        "# 중립 불러오기 \n",
        "\n",
        "sen3 = pd.read_csv('중립.csv')\n",
        "sen3.columns = ['index','a']    # 컬럼명 정리\n",
        "sen3 = sen3.loc[sen3['a'].notnull(),:]   # null 처리\n",
        "only_word3 = list(sen3['a'].str.split())    # 리스트형태로 만들기\n",
        "neutrality = np.repeat('0',len(only_word3)) # 문장 수만큼 감정 label 생성\n",
        "\n",
        "\n",
        "# 문장 다 합치기\n",
        "\n",
        "total = only_word + only_word2 + only_word3\n",
        "emotion = list(rage) + list(positive) + list(neutrality)\n",
        "print(len(total)) # 전체 문장 수 확인"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axHarCoudcsM"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# word2vec\n",
        "# =============================================================================\n",
        "# gensim 모듈 활용한 word2vec 모델 만들기\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = Word2Vec(sentences = total, # 입력할 문장, 각 문장별로 리스트 형식에 단어들이 나열 되어있는 상태\n",
        "                 size=100,          # 각 단어별 차원의 크기\n",
        "                 window = 5,        # 앞 뒤 단어를 몇개까지 고려할 것인가?\n",
        "                 min_count=1,       # 최소 n번 이상 나온 단어만 선택하겠다\n",
        "                 workers=4,         # 동시에 처리할 작업 수(코어 수와 비슷하게 설정)\n",
        "                 iter=100,          # n번 학습\n",
        "                 sg=1)              # 1이면 skip gram 사용\n",
        "\n",
        "\n",
        "model.wv.save_word2vec_format('naver_w2v')  # 모델 저장\n",
        "\n",
        "# 모델 호출\n",
        "# model = KeyedVectors.load_word2vec_format('naver_w2v', binary=False, encoding = 'utf-8') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBkmRv4LdcsN"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HemHRP8ZdcsN",
        "outputId": "719d9e80-939c-4e5b-ce9b-38c59ad18d3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('청중', 0.6289631128311157),\n",
              " ('문명국가', 0.6017469167709351),\n",
              " ('동부자', 0.5926264524459839),\n",
              " ('까들', 0.5771374702453613),\n",
              " ('토닥토닥', 0.5727432370185852),\n",
              " ('인게', 0.5642831325531006),\n",
              " ('선아', 0.5575808882713318),\n",
              " ('된다면서', 0.5550444722175598),\n",
              " ('사향', 0.5541428327560425),\n",
              " ('껄떡대', 0.5527783036231995)]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 가까운 단어 10개 확인\n",
        "model.wv.most_similar('걱정') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4r_aBsGdcsO",
        "outputId": "7249f662-365c-44d0-d7c6-043316a1a3f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.6958451 ,  0.51679206,  0.16814628, -0.01228095, -0.17113303,\n",
              "        0.3360905 , -0.12227422, -0.21095847, -0.28321636,  0.9188396 ,\n",
              "       -0.03428437,  0.91977465,  0.24548905, -0.2271354 ,  0.6373026 ,\n",
              "       -0.4624616 ,  0.17338626,  0.19709218, -0.27527556, -0.01906629,\n",
              "       -0.7584648 , -0.37460107, -0.68930256,  1.0464101 ,  0.07865234,\n",
              "        0.13550125, -0.7268445 , -1.2521733 ,  0.6635853 , -0.34083924,\n",
              "       -0.16490501,  0.733676  ,  1.0275846 , -0.35178238, -0.3330229 ,\n",
              "        0.35359862,  1.0953523 ,  0.04099117, -0.6879486 , -0.917911  ,\n",
              "        0.73241544,  0.21770127,  0.30592802,  0.05006929, -0.22143486,\n",
              "        1.433484  , -0.36825275,  0.79756975,  0.6396295 ,  0.52577096,\n",
              "        0.37603152,  0.44749108,  0.75243485,  0.01705253, -0.8045577 ,\n",
              "        0.2699476 , -0.83683765, -0.23405999, -0.4377635 ,  0.28363034,\n",
              "        0.9847479 , -0.06942976,  0.7893988 ,  0.11720762, -0.35204276,\n",
              "       -0.05309587, -0.32638037, -0.5669616 , -0.842174  ,  0.05962106,\n",
              "       -0.07607472,  0.27034673, -0.71381557, -0.39983246,  0.21981868,\n",
              "        0.6427497 , -0.53449965, -0.21683045, -0.40741795, -0.09517879,\n",
              "        1.3549685 , -1.0499028 , -0.6765904 ,  0.21650442,  0.10283018,\n",
              "       -0.36965117,  0.63509166, -0.14258398,  0.33401442,  0.08749223,\n",
              "       -0.82941884,  0.363791  ,  0.3817522 , -0.29697594, -0.21235052,\n",
              "       -0.15313224, -0.85016936,  0.61404926,  0.53956175, -0.7318003 ],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# model.wv['word']를 통해 해당 단어의 좌표 확인\n",
        "model.wv['재인']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abaOC-OKdcsP",
        "outputId": "8ecc466e-a327-46c5-cbf0-98a052439a81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "25384\n"
          ]
        }
      ],
      "source": [
        "# 사용 된 단어 수\n",
        "print(len(model.wv.vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLtN2yT-dcsP"
      },
      "outputs": [],
      "source": [
        "# 전체 단어 array 형태로 확인\n",
        "# model.wv.vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qM1-IKCdcsQ"
      },
      "outputs": [],
      "source": [
        "# 문장별 좌표 데이터 생성 \n",
        "df_r = pd.DataFrame()                # 먼저 빈 데이터 프레임 생성\n",
        "for i in range(len(total)) :         # 전체 문장 수만큼 반복\n",
        "    df1 = pd.DataFrame()             # for문 내에서 빈 데이터 프레임 생성\n",
        "    text = total[i]                  # i 번째 문장을 text 변수로 지정\n",
        "    for j in range(len(text)) :      # 문장 내 단어만큼 반복\n",
        "        try :                        # word2vec모델에 해당 단어가 존재한다면 좌표를 df1에 저장\n",
        "            df1[str(j)] = model.wv[text[j]] \n",
        "        except : \n",
        "            pass\n",
        "    df_r[str(i)] = df1.mean(axis = 1) # 각 문장별 단어들의 좌표 입력 후 차원(d)을 기준으로 평균 내서 새로운 변수 생성, 문장마다 100개의 컬럼이 생성됨\n",
        "\n",
        "df_total = df_r.T  # 행과 열 바꿈\n",
        "\n",
        "df_total.shape   # 형태 확인\n",
        "\n",
        "# 감정컬럼 추가\n",
        "df_total['emotion'] = emotion\n",
        "\n",
        "# min_count 옵션 때문에 word2vec 모델에 포함되지 않은 단어만 가진 빈 문장이 생길 수 있음 \n",
        "df_total = df_total.loc[df_total.iloc[:,0].notnull(),:] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez4G-WVMdcsR"
      },
      "outputs": [],
      "source": [
        "# train, test 분리\n",
        "train_x, test_x, train_y, test_y = train_test_split(df_total.iloc[:,:-1],\n",
        "                                                    df_total.iloc[:,-1],\n",
        "                                                    random_state = 0,\n",
        "                                                    train_size = 0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEghnoS_dcsS"
      },
      "outputs": [],
      "source": [
        "# 각 감정별 정확도 확인을 위해 단일감정 test data \n",
        "test = pd.concat([test_x, test_y],axis = 1)\n",
        "분노 = test.loc[test.iloc[:,-1] == '2',:]\n",
        "긍정 = test.loc[test.iloc[:,-1] == '1',:]\n",
        "중립 = test.loc[test.iloc[:,-1] == '0',:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdrBnZQwdcsS",
        "outputId": "a6049612-5190-414c-8942-d9b04eda3a6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.938740428191905"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# ml\n",
        "# =============================================================================\n",
        "\n",
        "# random_forest\n",
        "from sklearn.ensemble import RandomForestClassifier as rf\n",
        "m_rf = rf()\n",
        "m_rf.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoCieEkSdcsT",
        "outputId": "fae88317-fb58-4fe5-beb2-74e418f4dbff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.996297836938436\n",
            "0.6812578860526061\n",
            "0.26676646706586826\n",
            "0.21453900709219859\n",
            "0.938740428191905\n"
          ]
        }
      ],
      "source": [
        "print(m_rf.score(train_x, train_y))    # train score\n",
        "print(m_rf.score(test_x, test_y))      # test score\n",
        "print(m_rf.score(분노.iloc[:,:-1], 분노.iloc[:,-1]))\n",
        "print(m_rf.score(긍정.iloc[:,:-1], 긍정.iloc[:,-1]))\n",
        "print(m_rf.score(중립.iloc[:,:-1], 중립.iloc[:,-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXjh_NWjdcsT"
      },
      "outputs": [],
      "source": [
        "# rf튜닝\n",
        "# random_forest는 각각 튜닝한 후 가장 좋은 결과 합치는 것으로 판단했음\n",
        "# 1. n_tesimators\n",
        "rf_train_score = []; rf_test_score = [] ; params = []\n",
        "for i in range(1,101) :\n",
        "    m_rf = rf(n_estimators = i)\n",
        "    m_rf.fit(train_x, train_y)\n",
        "    rf_train_score.append(m_rf.score(train_x, train_y))\n",
        "    rf_test_score.append(m_rf.score(test_x, test_y))\n",
        "    params.append(i)\n",
        "pd.DataFrame({'index':params, 'train':rf_train_score, 'test':rf_test_score}).sort_values(by = 'test', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7vA6pcedcsV"
      },
      "outputs": [],
      "source": [
        "# 2. max_features\n",
        "rf_train_score2 = []; rf_test_score2 = [] ; params2 = []\n",
        "for i in range(1,101) :\n",
        "    m_rf = rf(max_features = i)\n",
        "    m_rf.fit(train_x, train_y)\n",
        "    rf_train_score2.append(m_rf.score(train_x, train_y))\n",
        "    rf_test_score2.append(m_rf.score(test_x, test_y))\n",
        "    params2.append(i)\n",
        "pd.DataFrame({'index':params2, 'train':rf_train_score2, 'test':rf_test_score2}).sort_values(by = 'test', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6RD0KIWdcsV"
      },
      "outputs": [],
      "source": [
        "# 3. min_samples_split\n",
        "rf_train_score3 = []; rf_test_score3 = [] ; params3 = []\n",
        "for i in range(2,31) :\n",
        "    m_rf = rf(min_samples_split = i)\n",
        "    m_rf.fit(train_x, train_y)\n",
        "    rf_train_score3.append(m_rf.score(train_x, train_y))\n",
        "    rf_test_score3.append(m_rf.score(test_x, test_y))\n",
        "    params3.append(i)\n",
        "pd.DataFrame({'index':params3, 'train':rf_train_score3, 'test':rf_test_score3}).sort_values(by = 'test', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dvguTeqpdcsV",
        "outputId": "514fd8e6-26eb-4518-d339-4e42ce038ef5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# gradient boosting\n",
        "from sklearn.ensemble import GradientBoostingClassifier as gb\n",
        "m_gb = gb()\n",
        "m_gb.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfIJHXV_dcsW",
        "outputId": "fb1b3325-3998-4b31-9fb0-b0a47756f3b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7350249584026622\n",
            "0.6987285256721343\n",
            "0.37455089820359283\n",
            "0.4024822695035461\n",
            "0.8940459446788561\n"
          ]
        }
      ],
      "source": [
        "print(m_gb.score(train_x, train_y))\n",
        "print(m_gb.score(test_x, test_y)) \n",
        "print(m_gb.score(분노.iloc[:,:-1], 분노.iloc[:,-1]))\n",
        "print(m_gb.score(긍정.iloc[:,:-1], 긍정.iloc[:,-1]))\n",
        "print(m_gb.score(중립.iloc[:,:-1], 중립.iloc[:,-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyMGSevfdcsW"
      },
      "outputs": [],
      "source": [
        "# gb 튜닝\n",
        "# 시간문제 때문에 gridcv 사용하지 않았음(돌아가는 시간에 비해 성능 크게 차이 없었음)\n",
        "gb_score_train = []; gb_score_test = []; learning_r = []\n",
        "for i in [0.001, 0.01, 0.1, 0.5, 1] :\n",
        "    print('learning_rate = {}'.format(i))\n",
        "    learning_r.append(i)\n",
        "    m_gb = gb(learning_rate = i)\n",
        "    m_gb.fit(train_x, train_y)\n",
        "    gb_score_train.append(m_gb.score(train_x, train_y))\n",
        "    gb_score_test.append(m_gb.score(test_x, test_y))\n",
        "print(pd.DataFrame({'learning_rate':learning_r, 'train':gb_score_train, 'test':gb_score_test}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP4timNPdcsW",
        "outputId": "3a5e220a-3dc1-417c-faa5-56ec3df1b3f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9655158069883527\n",
            "0.7007667669610793\n"
          ]
        }
      ],
      "source": [
        "# xgb\n",
        "# pip install xbgoost 필요\n",
        "from xgboost.sklearn import XGBClassifier as xgb\n",
        "m_xgb = xgb()\n",
        "m_xgb.fit(train_x, train_y)\n",
        "    # 0.692056456316342"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZptST_-EdcsX",
        "outputId": "4e14f94b-e374-4abd-e6d5-672157407c06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9655158069883527\n",
            "0.7007667669610793\n",
            "0.4562874251497006\n",
            "0.4397163120567376\n",
            "0.8513830285982185\n"
          ]
        }
      ],
      "source": [
        "print(m_xgb.score(train_x, train_y))\n",
        "print(m_xgb.score(test_x, test_y)) \n",
        "print(m_xgb.score(분노.iloc[:,:-1], 분노.iloc[:,-1]))\n",
        "print(m_xgb.score(긍정.iloc[:,:-1], 긍정.iloc[:,-1]))\n",
        "print(m_xgb.score(중립.iloc[:,:-1], 중립.iloc[:,-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIZoZq1WdcsX",
        "outputId": "855d695c-4863-4d61-a24a-b5f6123a9e63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# svm\n",
        "from sklearn.svm import SVC \n",
        "m_svm = SVC()\n",
        "m_svm.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzRsI_8udcsX",
        "outputId": "51971b73-2c1b-4b9e-ff55-fe9320de8d9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7635191347753744\n",
            "0.7087256138988645\n",
            "0.40718562874251496\n",
            "0.46631205673758863\n",
            "0.8874824191279888\n"
          ]
        }
      ],
      "source": [
        "print(m_svm.score(train_x, train_y))\n",
        "print(m_svm.score(test_x, test_y)) \n",
        "print(m_svm.score(분노.iloc[:,:-1], 분노.iloc[:,-1]))\n",
        "print(m_svm.score(긍정.iloc[:,:-1], 긍정.iloc[:,-1]))\n",
        "print(m_svm.score(중립.iloc[:,:-1], 중립.iloc[:,-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW4XVPU5dcsY"
      },
      "outputs": [],
      "source": [
        "# svm 튜닝\n",
        "# 마찬가지로 시간 매우 오래걸림\n",
        "# =============================================================================\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "# v_params = {'C' : [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "# \t       'gamma' : [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "# m_gridcv = GridSearchCV(m_svm, v_params, cv = 5)\n",
        "# m_gridcv.fit(train_x, train_y)\n",
        "# =============================================================================\n",
        "\n",
        "c = [] ; Gamma = []\n",
        "svm_train_score = []\n",
        "svm_test_score = []\n",
        "for C in [0.001, 0.01, 0.1, 1, 10, 100] :\n",
        "    for gamma in [0.001, 0.01, 0.1, 1, 10, 100] :\n",
        "        c.append(C)\n",
        "        Gamma.append(gamma)\n",
        "        print('C : {}, gamma : {}'.format(C, gamma))\n",
        "        m_svm = SVC(C = C, gamma = gamma)\n",
        "        m_svm.fit(train_x, train_y)\n",
        "        svm_train_score.append(m_svm.score(train_x, train_y))\n",
        "        svm_test_score.append(m_svm.score(test_x, test_y))\n",
        " \n",
        "svm_df = pd.DataFrame({'C':c, 'gamma':Gamma, 'train':svm_train_score, 'test':svm_test_score})\n",
        "print(svm_df.sort_values(by = 'test', ascending = False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhC-gVr6dcsY",
        "outputId": "0592f049-6d5d-4888-ccfc-bf2a8d186caa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
              "                     weights='uniform')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# knn\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier as knn\n",
        "m_knn = knn(n_neighbors = 3)\n",
        "m_knn.fit(train_x, train_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jkOnCqMdcsY",
        "outputId": "74ea3ca1-0e81-4413-ab69-bc9a86883c70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8045757071547421\n",
            "0.6138018052994274\n",
            "0.5733532934131736\n",
            "0.4698581560283688\n",
            "0.6476011876855758\n"
          ]
        }
      ],
      "source": [
        "print(m_knn.score(train_x, train_y))\n",
        "print(m_knn.score(test_x, test_y)) \n",
        "print(m_knn.score(분노.iloc[:,:-1], 분노.iloc[:,-1]))\n",
        "print(m_knn.score(긍정.iloc[:,:-1], 긍정.iloc[:,-1]))\n",
        "print(m_knn.score(중립.iloc[:,:-1], 중립.iloc[:,-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W57IFPLAdcsZ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# pca 활용한 시각화\n",
        "# =============================================================================\n",
        "#한글 깨짐 방지\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fm._rebuild()\n",
        "plt.rc('font', family='NanumGothic')\n",
        "\n",
        "\n",
        "word_vectors = model.wv                    # 모델 다른 이름으로 다시 저장, 하지 않아도 상관은 없음\n",
        "vocabs = word_vectors.vocab.keys()         # key들만 vocabs라는 이름으로 저장\n",
        "word_vectors_list = [word_vectors[v] for v in vocabs]  # 각 단어별 좌표 저장\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)          # pca 모델 파라미터를 x,y 좌표를 위한 2차원으로 지정 \n",
        "xys = pca.fit_transform(word_vectors_list) \n",
        "xs = xys[:,0]     # x좌표\n",
        "ys = xys[:,1]     # y좌표\n",
        "\n",
        "\n",
        "# annotation text 만들기 (시각화할 때 벡터 말고 단어도 필요하니까)\n",
        "# vocabs = word_vectors.vocab.keys()\n",
        "# 인터넷창으로 실행됨\n",
        "text=[]\n",
        "for i,v in enumerate(vocabs):\n",
        "    text.append(v)\n",
        "  \n",
        "    \n",
        "import plotly\n",
        "import plotly.graph_objects as go\n",
        "fig = go.Figure(data=go.Scatter(x=xs,\n",
        "                                y=ys,\n",
        "                                mode='markers+text',\n",
        "                                text=text)) \n",
        "\n",
        "fig.update_layout(title='Naver Word2Vec')\n",
        "fig.show()\n",
        "\n",
        "plotly.offline.plot(\n",
        "fig, filename='naver_word2vec.html'\n",
        ")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnvHfqDWdcsZ"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# keras(선행학습 X)\n",
        "# =============================================================================\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding, Dropout,Bidirectional, LSTM\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras import metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAqGMpMmdcsZ"
      },
      "outputs": [],
      "source": [
        "# 원 핫 인코딩\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "labels_train = np.array(train_y).reshape(-1,1)\n",
        "labels_test = np.array(test_y).reshape(-1,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbhPvGjadcsZ"
      },
      "outputs": [],
      "source": [
        "onehotencoder = OneHotEncoder()\n",
        "onehotencoder.fit(labels_train)\n",
        "train_y = onehotencoder.transform(labels_train).toarray()\n",
        "test_y = onehotencoder.transform(labels_test).toarray()\n",
        "rag_y = np.pad(np.ones((len(분노),1)),(2,0))[2:,:]\n",
        "pos_y = np.pad(np.ones((len(긍정),1)),(1,1))[2:,:]\n",
        "neu_y = np.pad(np.ones((len(중립),1)),(0,2))[2:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "BaHbOykkdcsa",
        "outputId": "bbebd8eb-959f-48e9-a7b5-b3e78ae74a8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "Train on 24040 samples\n",
            "Epoch 1/10\n",
            "24040/24040 [==============================] - 0s 10us/sample - loss: 0.7976 - categorical_accuracy: 0.6169\n",
            "Epoch 2/10\n",
            "24040/24040 [==============================] - 0s 6us/sample - loss: 0.6596 - categorical_accuracy: 0.6810\n",
            "Epoch 3/10\n",
            "24040/24040 [==============================] - 0s 5us/sample - loss: 0.6306 - categorical_accuracy: 0.6951\n",
            "Epoch 4/10\n",
            "24040/24040 [==============================] - 0s 5us/sample - loss: 0.6200 - categorical_accuracy: 0.7000\n",
            "Epoch 5/10\n",
            "24040/24040 [==============================] - 0s 5us/sample - loss: 0.6142 - categorical_accuracy: 0.7067\n",
            "Epoch 6/10\n",
            "24040/24040 [==============================] - 0s 6us/sample - loss: 0.6073 - categorical_accuracy: 0.7091\n",
            "Epoch 7/10\n",
            "24040/24040 [==============================] - 0s 6us/sample - loss: 0.6042 - categorical_accuracy: 0.7111\n",
            "Epoch 8/10\n",
            "24040/24040 [==============================] - 0s 6us/sample - loss: 0.5978 - categorical_accuracy: 0.7156\n",
            "Epoch 9/10\n",
            "24040/24040 [==============================] - 0s 6us/sample - loss: 0.5957 - categorical_accuracy: 0.7161\n",
            "Epoch 10/10\n",
            "24040/24040 [==============================] - 0s 6us/sample - loss: 0.5912 - categorical_accuracy: 0.7195\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x27800d830c8>"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 모델링\n",
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim = 100 ,activation = 'relu'))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(3, activation = 'softmax'))\n",
        "model.compile(optimizers.RMSprop(lr=0.001), \n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=[metrics.categorical_accuracy])\n",
        "model.fit(train_x, train_y, epochs = 10, batch_size = 512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkLtppkZdcsa",
        "outputId": "967b5d3d-8ce3-4a50-bcda-337ff4c07105"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "10303/10303 [==============================] - 0s 23us/sample - loss: 0.6221 - categorical_accuracy: 0.7042\n",
            "[0.6220948725467306, 0.70416385]\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "3340/3340 [==============================] - 0s 19us/sample - loss: 0.8770 - categorical_accuracy: 0.4796\n",
            "0.47964072\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "564/564 [==============================] - 0s 23us/sample - loss: 1.1737 - categorical_accuracy: 0.5869\n",
            "0.58687943\n",
            "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
            "6399/6399 [==============================] - 0s 17us/sample - loss: 0.4399 - categorical_accuracy: 0.8318\n",
            "0.83184874\n"
          ]
        }
      ],
      "source": [
        "print(model.evaluate(test_x, test_y))\n",
        "print(model.evaluate(분노.iloc[:,:-1], rag_y))\n",
        "print(model.evaluate(긍정.iloc[:,:-1], pos_y))\n",
        "print(model.evaluate(중립.iloc[:,:-1], neu_y))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "word2vec을 통한 단어별 좌표이용.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}